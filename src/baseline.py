# -*- coding: utf-8 -*-
"""Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AvuYXDOm8KsbEfpunJUVjPHFGlxV2F56
"""

import tensorflow as tf
import tensorflow_hub as hub

import json
import spacy
import numpy as np
import pandas as pd
import gzip
from sklearn.mixture import GaussianMixture
from sklearn.mixture import BayesianGaussianMixture
from sklearn.metrics.pairwise import cosine_similarity

def parse(path):
    g = gzip.open(path, 'rb')
    for l in g:
        yield eval(l)

def getDF(path):
    i = 0
    df = {}
    for d in parse(path):
        df[i] = d
        i += 1
    return pd.DataFrame.from_dict(df, orient='index')

from google.colab import drive
drive.mount('/content/drive')

#have dataset in google cloud storage
df = getDF('/content/drive/My Drive/reviews_CDs_and_Vinyl_5.json.gz')

"""# Baseline Experiments
## Review level

### Get review text from product with maximum number of reviews
"""

products = df.groupby('asin')
popular_product = products.get_group(max([(name,len(g)) for name, g in products], key=lambda x: x[1])[0])
text = popular_product.reviewText.values.tolist()

"""### Get the embeddings"""

module_url = "https://tfhub.dev/google/universal-sentence-encoder/2"
embed = hub.Module(module_url)

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(embed(text))

message_embeddings

"""### Cluster"""

gmm = GaussianMixture(n_components=5, covariance_type='full').fit(message_embeddings)
bgmm = BayesianGaussianMixture(n_components=5, covariance_type='full').fit(message_embeddings)

print(gmm.means_)
print("\n")
print(bgmm.means_)

"""### Extract"""

sim1 = cosine_similarity(message_embeddings, gmm.means_)
sim2 = cosine_similarity(message_embeddings, bgmm.means_)

extractive_means1 = sim1.argmax(axis=0)
extractive_means2 = sim2.argmax(axis=0)

for i in extractive_means1:
  print(text[i] + '\n')
  
print("##########")
  
for i in extractive_means2:
  print(text[i] + '\n')

"""## Sentence Level"""

spacy.prefer_gpu()
nlp = spacy.load('en_core_web_sm')

"""### Perform sentence segmentation"""

sentences = []
for review in text:
  doc = nlp(review)
  for sent in doc.sents:
    sentences.append(sent.text)

sentences[:10]

"""### Proceed as before"""

review_sentence_embeddings = []
with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  review_sentence_embeddings = session.run(embed(sentences))

gmm_sentence = GaussianMixture(n_components=10, covariance_type='full').fit(review_sentence_embeddings)
bgmm_sentence = BayesianGaussianMixture(n_components=10, covariance_type='full').fit(review_sentence_embeddings)

sim1 = cosine_similarity(review_sentence_embeddings, gmm_sentence.means_)
extractive_means1 = sim1.argmax(axis=0)

sim2 = cosine_similarity(review_sentence_embeddings, bgmm_sentence.means_)
extractive_means2 = sim2.argmax(axis=0)

for i in extractive_means1:
  print(sentences[i] + '\n')

print('###########')
  
for i in extractive_means2:
  print(sentences[i] + '\n')

